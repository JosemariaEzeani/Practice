**Neural Networks**
1. **Autoencoder**
   1. Bottleneck layer 
   2. denoising autoencoder
2. **DeepDream**
3. **Multilayer perceptron**
4. Unit
5. Layer
6. Fully-Connected
7. Activation Functions:
   1. TanH
   2. ReLU
8. Backpropagation
9. **RNN**
   1. **LSTM**
   2. **GRU**
   3. **ESN**
10. **Restricted Boltzmann machine**
11. **GAN**
12. **SOM**
13. **Convolutional neural network** 
    1. **U-Net**
    2. stride 
    3. Padding 
    4. Pooling 
14. **Transformer**
15. **Feed-Forward**
16. **Extreme Learning**
17. **Logic**
18. **Self-Organising Map** 
19. Skip connections 
20. residual neural networks 
21. **Deep Learning**: more than two non-output layers
    1. **Early Challenges:**
       1. **Exploding Gradient:** solution = gradient clipping + L1 + L2 regularisation
       2. **Vanishing Gradient**
    2. **Deep belief networks** 
    3. **Deep Boltzmann machines** 
    4. **Deep Convolutional neural networks** 
    5. **Deep Recurrent neural networks** 
    6. **Hierarchical temporal memory** 
    7. **Generative Adversarial Networks** 
    8. **Deep Boltzmann Machine (DBM)**
    9. **Stacked Auto-Encoders**
    10. **Convolutional neural Network** 
    11. **Recurrent Neural Network** 
        1. gated rnn
        2. Minimal gated unit 
        3. gated recurrent unit 
        4. **Long short-term memory (LSTM)**
        5. Softmax 
        6. Bi-directional 
        7. Attention 
        8. Sequence-to-sequence
