**Important Topics:** 

1. **Objectitve Function:** expression minimised/maximised in a learning algorithm.
   1. **(Stochastic) Gradient Descent:** 
      1. **Minibatch:**
         1. **Adagrad:**
         2. **Momentum:**
      2. **RMSprop**
      3. **Adam**
   2. **Cost Function:**
   3. **Loss Function:**
      1. **Squared Error Loss:**
      2. **Binary Loss:**
      3. **Hinge Loss:**
      4. **Average Loss/ Empirical Risk**
2. **Cross-Validation** 
3. **Feature Engineering**
   1. **One-hot Encoding** 
   2. **Binning/Bucketing**
   3. **Normalisation**
   4. **Standardisation (Z-score Normalisation)**
   5. **Missing Features** 
      6. **Data Imputation** 
4. **Bias-Variance Tradeoff**
    5. **Underfitting:** high bias/high variance?.
    2. **Overfitting:** low bias/high variance.
    3. **Regularisation**
        1. **Least Absolute Shrinkage and Selection Operator (LASSO/L1) regularisation:** produces sparce models via feature selection.
        2. **Ridge/L2 regularisation:** 
        3. **Elastic net Regularisation:**  L1 + L2 regularisation 
        4. Dropout
        5. Batch Normalisation
        6. Data Augmentation
        7. Early Stopping
        8. **Least-angle regression (LARS)**
7. **Hyperparameters:** variables set by data analysts before running a learning algorithm.
    1. **Tuning** 
